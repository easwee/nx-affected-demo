"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.defaultFileHasher = exports.FileHasher = void 0;
const app_root_1 = require("@nrwl/tao/src/utils/app-root");
const perf_hooks_1 = require("perf_hooks");
const git_hasher_1 = require("./git-hasher");
const hashing_impl_1 = require("./hashing-impl");
class FileHasher {
    constructor(hashing) {
        this.hashing = hashing;
        this.fileHashes = {};
        this.workspaceFiles = new Set();
        this.usesGitForHashing = false;
        this.isInitialized = false;
    }
    clear() {
        this.fileHashes = {};
        this.workspaceFiles = new Set();
        this.usesGitForHashing = false;
        this.isInitialized = false;
    }
    /**
     * For the project graph daemon server use-case we can potentially skip expensive work
     * by leveraging knowledge of the uncommitted and untracked files, so the init() method
     * returns a Map containing this data.
     */
    init() {
        perf_hooks_1.performance.mark('init hashing:start');
        this.clear();
        const getFileHashesResult = (0, git_hasher_1.getFileHashes)(app_root_1.appRootPath);
        this.applyFileHashes(getFileHashesResult.allFiles);
        this.usesGitForHashing = Object.keys(this.fileHashes).length > 0;
        this.isInitialized = true;
        perf_hooks_1.performance.mark('init hashing:end');
        perf_hooks_1.performance.measure('init hashing', 'init hashing:start', 'init hashing:end');
        return getFileHashesResult.untrackedUncommittedFiles;
    }
    incrementalUpdate(updatedFiles, deletedFiles = []) {
        this.ensureInitialized();
        perf_hooks_1.performance.mark('incremental hashing:start');
        updatedFiles.forEach((hash, filename) => {
            this.fileHashes[filename] = hash;
            /**
             * we have to store it separately because fileHashes can be modified
             * later on and can contain files that do not exist in the workspace
             */
            this.workspaceFiles.add(filename);
        });
        for (const deletedFile of deletedFiles) {
            delete this.fileHashes[deletedFile];
            this.workspaceFiles.delete(deletedFile);
        }
        perf_hooks_1.performance.mark('incremental hashing:end');
        perf_hooks_1.performance.measure('incremental hashing', 'incremental hashing:start', 'incremental hashing:end');
    }
    hashFile(path) {
        this.ensureInitialized();
        const relativePath = path.startsWith(app_root_1.appRootPath)
            ? path.substr(app_root_1.appRootPath.length + 1)
            : path;
        if (!this.fileHashes[relativePath]) {
            this.fileHashes[relativePath] = this.processPath(path);
        }
        return this.fileHashes[relativePath];
    }
    ensureInitialized() {
        if (!this.isInitialized) {
            this.init();
        }
    }
    applyFileHashes(allFiles) {
        const sliceIndex = app_root_1.appRootPath.length + 1;
        allFiles.forEach((hash, filename) => {
            this.fileHashes[filename.substr(sliceIndex)] = hash;
            /**
             * we have to store it separately because fileHashes can be modified
             * later on and can contain files that do not exist in the workspace
             */
            this.workspaceFiles.add(filename.substr(sliceIndex));
        });
    }
    processPath(path) {
        try {
            return this.hashing.hashFile(path);
        }
        catch (_a) {
            return '';
        }
    }
}
exports.FileHasher = FileHasher;
exports.defaultFileHasher = new FileHasher(hashing_impl_1.defaultHashing);
//# sourceMappingURL=file-hasher.js.map